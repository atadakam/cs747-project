{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifacts.mitre.org/artifactory/api/pypi/python/simple\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# visulization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import gc # garbage collection\n",
    "import glob # extract path via pattern matching\n",
    "import random\n",
    "import math\n",
    "import cv2 # read image\n",
    "# store to disk\n",
    "import pickle\n",
    "import h5py # like numpy array\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPool2D, AveragePooling2D\n",
    "from keras.layers import Flatten, Dropout, BatchNormalization, Activation\n",
    "from keras.layers import Add\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import regularizers\n",
    "import keras\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = './state-farm-distracted-driver-detection/'\n",
    "TRAIN_DIR = ROOT_DIR + 'imgs/train/'\n",
    "TEST_DIR = ROOT_DIR + 'imgs/test/'\n",
    "driver_imgs_list = pd.read_csv(ROOT_DIR + \"driver_imgs_list.csv\")\n",
    "sample_submission = pd.read_csv(ROOT_DIR + \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path, img_height=None, img_width=None, rotate=False, color_type=0):\n",
    "    img = cv2.imread(path, color_type)\n",
    "    if img_width and img_height:\n",
    "        img = cv2.resize(img, (img_width, img_height))\n",
    "    if rotate is True:\n",
    "        rows, cols = img.shape\n",
    "        rotation_angle = random.uniform(10,-10)\n",
    "        M = cv2.getRotationMatrix2D((cols/2, rows/2), rotation_angle, 1)\n",
    "        img = cv2.warpAffine(img, M, (cols,rows))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(480, 640)}\n"
     ]
    }
   ],
   "source": [
    "random_list = np.random.permutation(len(driver_imgs_list))[:50]\n",
    "df_copy = driver_imgs_list.iloc[random_list]\n",
    "image_paths = [TRAIN_DIR+row.classname+'/'+row.img \n",
    "                   for (index, row) in df_copy.iterrows()]\n",
    "image_shapes = [get_image(path).shape for path in image_paths]\n",
    "print(set(image_shapes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_list = []\n",
    "label_list = []\n",
    "for index, row in driver_imgs_list.iterrows():\n",
    "    img_path_list.append('{0}{1}/{2}'.format(TRAIN_DIR, row.classname, row.img))\n",
    "    label_list.append(int(row.classname[1]))\n",
    "# One hot vector representation of labels\n",
    "y_labels_one_hot = to_categorical(label_list, dtype=np.int8)\n",
    "x_img_path = np.array(img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "np.save('x_img_path.npy', x_img_path)\n",
    "np.save('y_labels_one_hot.npy', y_labels_one_hot)\n",
    "\n",
    "x_img_path_shuffled, y_labels_one_hot_shuffled = shuffle(x_img_path, y_labels_one_hot)\n",
    "\n",
    "# saving the shuffled file.\n",
    "# you can load them later using np.load().\n",
    "np.save('y_labels_one_hot_shuffled.npy', y_labels_one_hot_shuffled)\n",
    "np.save('x_img_path_shuffled.npy', x_img_path_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17939,)\n",
      "(17939, 10)\n",
      "(4485,)\n",
      "(4485, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Used this line as our filename array is not a numpy array.\n",
    "x_img_path_shuffled_numpy = np.array(x_img_path_shuffled)\n",
    "\n",
    "X_train_filenames, X_val_filenames, y_train, y_val = train_test_split(\n",
    "    x_img_path_shuffled_numpy, y_labels_one_hot_shuffled, test_size=0.2, random_state=1)\n",
    "\n",
    "print(X_train_filenames.shape) # (3800,)\n",
    "print(y_train.shape)           # (3800, 12)\n",
    "\n",
    "print(X_val_filenames.shape)   # (950,)\n",
    "print(y_val.shape)             # (950, 12)\n",
    "\n",
    "# You can save these files as well. As you will be using them later for training and validation of your model.\n",
    "np.save('X_train_filenames.npy', X_train_filenames)\n",
    "np.save('y_train.npy', y_train)\n",
    "\n",
    "np.save('X_val_filenames.npy', X_val_filenames)\n",
    "np.save('y_val.npy', y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "BATCH_SIZE = 32\n",
    "CHANNEL = 3\n",
    "class Img_Generator(keras.utils.Sequence):\n",
    "    def __init__(self, image_filenames, labels, batch_size) :\n",
    "        self.image_filenames = image_filenames\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int)\n",
    "    def __getitem__(self, idx) :\n",
    "        batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        img_list = []\n",
    "        for file_name in batch_x:\n",
    "            if CHANNEL == 1:\n",
    "                original_img = cv2.imread(file_name, 0)\n",
    "            else:\n",
    "                original_img = cv2.imread(file_name, 1)\n",
    "            im = cv2.resize(original_img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "            #color = [0, 0, 0]\n",
    "            #new_im = cv2.copyMakeBorder(im, 40, 40, 0, 0, cv2.BORDER_CONSTANT, value=color)\n",
    "            #im = cv2.resize(new_im, (224, 224))\n",
    "            img_list.append(im)\n",
    "        img_batch = np.array(img_list)\n",
    "        if CHANNEL == 1:\n",
    "            img_batch = np.expand_dims(img_batch, axis=-1)\n",
    "        return img_batch, np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of input [BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH, CHANNEL]\n",
    "train_gen = Img_Generator(X_train_filenames, y_train, BATCH_SIZE)\n",
    "val_gen = Img_Generator(X_val_filenames, y_val, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f saved_models/weights_best_efficient.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from efficientnet.keras import EfficientNetB5\n",
    "from keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n",
    "                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D,concatenate,Concatenate,multiply, LocallyConnected2D, Lambda)\n",
    "\n",
    "nb_train_samples = 17943\n",
    "nb_validation_samples = 4481\n",
    "\n",
    "IMAGENET_WEIGHTS_HASHES = {\n",
    "    'efficientnet-b5': ('30172f1d45f9b8a41352d4219bf930ee'\n",
    "                        '3339025fd26ab314a817ba8918fefc7d',\n",
    "                        '9d197bc2bfe29165c10a2af8c2ebc675'\n",
    "                        '07f5d70456f09e584c71b822941b1952')\n",
    "}\n",
    "IMAGENET_WEIGHTS_PATH = (\n",
    "    'https://github.com/Callidior/keras-applications/'\n",
    "    'releases/download/efficientnet/')\n",
    "\n",
    "eff_net = EfficientNetB5(weights=None,\n",
    "                        include_top=False,\n",
    "                        input_shape=(IMG_WIDTH, IMG_HEIGHT, CHANNEL))\n",
    "\n",
    "in_lay = Input(shape=(64,64,3))\n",
    "\n",
    "file_name = \"efficientnet-b5\" + '_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5'\n",
    "file_hash = IMAGENET_WEIGHTS_HASHES[\"efficientnet-b5\"][1]\n",
    "weights_path = keras.utils.get_file(\n",
    "            file_name,\n",
    "            IMAGENET_WEIGHTS_PATH + file_name,\n",
    "            cache_subdir='models',\n",
    "            file_hash=file_hash,\n",
    " )\n",
    "\n",
    "eff_net.load_weights(weights_path)\n",
    "pt_depth = eff_net.get_output_shape_at(0)[-1]\n",
    "pt_features = eff_net(in_lay)\n",
    "bn_features = BatchNormalization()(pt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n",
    "                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D,concatenate,Concatenate,multiply, LocallyConnected2D, Lambda)\n",
    "# Attention mechanism to turn pixels in the GAP on and off\n",
    "attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(Dropout(0.5)(bn_features))\n",
    "attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
    "attn_layer = Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
    "attn_layer = Conv2D(1, kernel_size = (1,1), padding = 'valid', activation = 'sigmoid')(attn_layer)\n",
    "\n",
    "mask_features = multiply([attn_layer, bn_features])\n",
    "gap_features = GlobalAveragePooling2D()(mask_features)\n",
    "gap_mask = GlobalAveragePooling2D()(attn_layer)\n",
    "# to account for missing values from the attention model\n",
    "gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n",
    "gap_dr = Dropout(0.25)(gap)\n",
    "dr_steps = Dropout(0.25)(Dense(128, activation = 'relu')(gap_dr))\n",
    "out_layer = Dense(10, activation = 'softmax')(dr_steps)\n",
    "retina_model = Model(inputs = [in_lay], outputs = [out_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 64, 64, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "efficientnet-b5 (Model)         (None, 2, 2, 2048)   28513520    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 2, 2, 2048)   8192        efficientnet-b5[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 2, 2, 2048)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 2, 2, 64)     131136      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 2, 2, 16)     1040        conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 2, 2, 8)      136         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 2, 2, 1)      9           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 2, 2, 2048)   0           conv2d_4[0][0]                   \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 1)            0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "RescaleGAP (Lambda)             (None, 2048)         0           global_average_pooling2d_1[0][0] \n",
      "                                                                 global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 2048)         0           RescaleGAP[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          262272      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           1290        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,917,595\n",
      "Trainable params: 28,740,763\n",
      "Non-trainable params: 176,832\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "retina_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/keras/callbacks/callbacks.py:998: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import (ModelCheckpoint, LearningRateScheduler,\n",
    "                             EarlyStopping, ReduceLROnPlateau,CSVLogger)\n",
    "\n",
    "epochs = 15; batch_size = 32\n",
    "checkpoint = ModelCheckpoint('./model_.h5', monitor='loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=4, \n",
    "                                   verbose=1, mode='auto', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=9)\n",
    "csv_logger = CSVLogger(filename='./training_log.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "\n",
    "train_generator = train_gen\n",
    "# train_mixup = My_Generator(train_x, train_y, batch_size, is_train=True, mix=False, augment=True)\n",
    "valid_generator = val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "class QWKEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), batch_size=64, interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.batch_size = batch_size\n",
    "        self.valid_generator, self.y_val = validation_data\n",
    "        self.history = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict_generator(generator=self.valid_generator,\n",
    "                                                  steps=np.ceil(float(len(self.y_val)) / float(self.batch_size)),\n",
    "                                                  workers=1, use_multiprocessing=False,\n",
    "                                                  verbose=1)\n",
    "            def flatten(y):\n",
    "                return np.argmax(y, axis=1).reshape(-1)\n",
    "            \n",
    "            score = cohen_kappa_score(flatten(self.y_val),\n",
    "                                      flatten(y_pred),\n",
    "                                      labels=[0,1,2,3,4],\n",
    "                                      weights='quadratic')\n",
    "            print(\"\\n epoch: %d - QWK_score: %.6f \\n\" % (epoch+1, score))\n",
    "            self.history.append(score)\n",
    "            if score >= max(self.history):\n",
    "                print('saving checkpoint: ', score)\n",
    "                self.model.save('model_bestqwk.h5')\n",
    "\n",
    "qwk = QWKEvaluation(validation_data=(valid_generator, y_val),\n",
    "                    batch_size=batch_size, interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "141/141 [==============================] - 55s 388ms/step - loss: 2.5187 - accuracy: 0.1849\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.51903, saving model to ./model_.h5\n",
      "Epoch 2/2\n",
      "141/141 [==============================] - 47s 330ms/step - loss: 2.2053 - accuracy: 0.2580\n",
      "\n",
      "Epoch 00002: loss improved from 2.51903 to 2.20531, saving model to ./model_.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a42a8cef0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in retina_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for i in range(-3,0):\n",
    "    retina_model.layers[i].trainable = True\n",
    "\n",
    "retina_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=Adam(1e-3),metrics=['accuracy'])\n",
    "\n",
    "retina_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(y_train)) / float(128)),\n",
    "    epochs=2,\n",
    "    workers=2, use_multiprocessing=True,\n",
    "    verbose=1,\n",
    "    callbacks=[early, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kappa_loss(y_true, y_pred, y_pow=2, eps=1e-12, N=10, bsize=32, name='kappa'):\n",
    "    with tf.name_scope(name):\n",
    "        y_true = tf.to_float(y_true)\n",
    "        repeat_op = tf.to_float(tf.tile(tf.reshape(tf.range(0, N), [N, 1]), [1, N]))\n",
    "        repeat_op_sq = tf.square((repeat_op - tf.transpose(repeat_op)))\n",
    "        weights = repeat_op_sq / tf.to_float((N - 1) ** 2)\n",
    "    \n",
    "        pred_ = y_pred ** y_pow\n",
    "        try:\n",
    "            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [-1, 1]))\n",
    "        except Exception:\n",
    "            pred_norm = pred_ / (eps + tf.reshape(tf.reduce_sum(pred_, 1), [bsize, 1]))\n",
    "    \n",
    "        hist_rater_a = tf.reduce_sum(pred_norm, 0)\n",
    "        hist_rater_b = tf.reduce_sum(y_true, 0)\n",
    "    \n",
    "        conf_mat = tf.matmul(tf.transpose(pred_norm), y_true)\n",
    "    \n",
    "        nom = tf.reduce_sum(weights * conf_mat)\n",
    "        denom = tf.reduce_sum(weights * tf.matmul(\n",
    "            tf.reshape(hist_rater_a, [N, 1]), tf.reshape(hist_rater_b, [1, N])) /\n",
    "                              tf.to_float(bsize))\n",
    "    \n",
    "        return nom*0.5 / (denom + eps) + categorical_crossentropy(y_true, y_pred)*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "561/561 [==============================] - 1346s 2s/step - loss: 0.3980 - val_loss: 0.0023\n",
      "\n",
      "Epoch 00001: val_loss improved from 0.03256 to 0.00233, saving model to ./model_.h5\n",
      "141/141 [==============================] - 59s 418ms/step\n",
      "\n",
      " epoch: 1 - QWK_score: 0.982859 \n",
      "\n",
      "saving checkpoint:  0.982858592150755\n",
      "Epoch 2/15\n",
      "561/561 [==============================] - 1270s 2s/step - loss: 0.1652 - val_loss: 1.3542e-05\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.00233 to 0.00001, saving model to ./model_.h5\n",
      "141/141 [==============================] - 55s 391ms/step\n",
      "\n",
      " epoch: 2 - QWK_score: 0.989366 \n",
      "\n",
      "saving checkpoint:  0.9893659354854508\n",
      "Epoch 3/15\n",
      "561/561 [==============================] - 1244s 2s/step - loss: 0.0876 - val_loss: 5.3644e-06\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00001 to 0.00001, saving model to ./model_.h5\n",
      "141/141 [==============================] - 1303s 9s/step\n",
      "\n",
      " epoch: 3 - QWK_score: 0.993664 \n",
      "\n",
      "saving checkpoint:  0.9936644811410007\n",
      "Epoch 4/15\n",
      "561/561 [==============================] - 1261s 2s/step - loss: 0.0641 - val_loss: 5.3167e-06\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00001 to 0.00001, saving model to ./model_.h5\n",
      "141/141 [==============================] - 57s 404ms/step\n",
      "\n",
      " epoch: 4 - QWK_score: 0.996568 \n",
      "\n",
      "saving checkpoint:  0.9965683712733848\n",
      "Epoch 5/15\n",
      "561/561 [==============================] - 1267s 2s/step - loss: 0.0458 - val_loss: 2.8610e-07\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00001 to 0.00000, saving model to ./model_.h5\n",
      "141/141 [==============================] - 55s 392ms/step\n",
      "\n",
      " epoch: 5 - QWK_score: 0.994095 \n",
      "\n",
      "Epoch 6/15\n",
      "561/561 [==============================] - 1249s 2s/step - loss: 0.0389 - val_loss: 1.2016e-05\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "141/141 [==============================] - 55s 388ms/step\n",
      "\n",
      " epoch: 6 - QWK_score: 0.990769 \n",
      "\n",
      "Epoch 7/15\n",
      "561/561 [==============================] - 1262s 2s/step - loss: 0.0206 - val_loss: 1.9073e-06\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00000\n",
      "141/141 [==============================] - 56s 398ms/step\n",
      "\n",
      " epoch: 7 - QWK_score: 0.998501 \n",
      "\n",
      "saving checkpoint:  0.9985008390544103\n",
      "Epoch 8/15\n",
      "561/561 [==============================] - 1266s 2s/step - loss: 0.0117 - val_loss: 5.0068e-07\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00000\n",
      "141/141 [==============================] - 55s 391ms/step\n",
      "\n",
      " epoch: 8 - QWK_score: 0.997536 \n",
      "\n",
      "Epoch 9/15\n",
      "561/561 [==============================] - 1263s 2s/step - loss: 0.0136 - val_loss: 9.5367e-07\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00000\n",
      "141/141 [==============================] - 55s 392ms/step\n",
      "\n",
      " epoch: 9 - QWK_score: 0.996787 \n",
      "\n",
      "Epoch 10/15\n",
      "561/561 [==============================] - 1266s 2s/step - loss: 0.0137 - val_loss: 5.8173e-06\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "141/141 [==============================] - 56s 401ms/step\n",
      "\n",
      " epoch: 10 - QWK_score: 0.997212 \n",
      "\n",
      "Epoch 11/15\n",
      "561/561 [==============================] - 1273s 2s/step - loss: 0.0078 - val_loss: 3.3379e-07\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.00000\n",
      "141/141 [==============================] - 55s 392ms/step\n",
      "\n",
      " epoch: 11 - QWK_score: 0.999893 \n",
      "\n",
      "saving checkpoint:  0.9998929046123244\n",
      "Epoch 12/15\n",
      "561/561 [==============================] - 1249s 2s/step - loss: 0.0057 - val_loss: 1.9073e-07\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to ./model_.h5\n",
      "141/141 [==============================] - 57s 406ms/step\n",
      "\n",
      " epoch: 12 - QWK_score: 0.999465 \n",
      "\n",
      "Epoch 13/15\n",
      "561/561 [==============================] - 1310s 2s/step - loss: 0.0059 - val_loss: 2.3842e-08\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to ./model_.h5\n",
      "141/141 [==============================] - 58s 414ms/step\n",
      "\n",
      " epoch: 13 - QWK_score: 0.998929 \n",
      "\n",
      "Epoch 14/15\n",
      "561/561 [==============================] - 1309s 2s/step - loss: 0.0061 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to ./model_.h5\n",
      "\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n",
      "141/141 [==============================] - 57s 406ms/step\n",
      "\n",
      " epoch: 14 - QWK_score: 0.999465 \n",
      "\n",
      "Epoch 15/15\n",
      "561/561 [==============================] - 1299s 2s/step - loss: 0.0070 - val_loss: 0.0000e+00\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.00000\n",
      "141/141 [==============================] - 58s 411ms/step\n",
      "\n",
      " epoch: 15 - QWK_score: 0.998501 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a88232ba8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from sklearn.metrics import f1_score, fbeta_score, cohen_kappa_score\n",
    "\n",
    "for layer in retina_model.layers:\n",
    "    layer.trainable = True\n",
    "callbacks_list = [checkpoint, csv_logger, reduceLROnPlat, early, qwk]\n",
    "retina_model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=Adam(lr=1e-4))\n",
    "retina_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=np.ceil(float(len(y_train)) / float(batch_size)),\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=np.ceil(float(len(y_val)) / float(batch_size)),\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    workers=1, use_multiprocessing=False,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
