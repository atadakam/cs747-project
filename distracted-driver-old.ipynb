{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.framework.op_def_registry' has no attribute 'register_op_list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4676f3f343bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery_reader_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/tensorflow/contrib/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"nt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster_resolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/tensorflow/contrib/cloud/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=line-too-long,wildcard-import,g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbigquery_reader_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcs_config_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/tensorflow/contrib/cloud/python/ops/bigquery_reader_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcloud\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_bigquery_reader_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/tensorflow/contrib/cloud/python/ops/gen_bigquery_reader_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;31m#   }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;31m# }\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m \u001b[0m_op_def_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_InitOpDefLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb\"\\n\\355\\001\\n\\016BigQueryReader\\032\\024\\n\\rreader_handle\\030\\007\\200\\001\\001\\\"\\027\\n\\tcontainer\\022\\006string\\032\\002\\022\\000\\\"\\031\\n\\013shared_name\\022\\006string\\032\\002\\022\\000\\\"\\024\\n\\nproject_id\\022\\006string\\\"\\024\\n\\ndataset_id\\022\\006string\\\"\\022\\n\\010table_id\\022\\006string\\\"\\027\\n\\007columns\\022\\014list(string)\\\"\\027\\n\\020timestamp_millis\\022\\003int\\\"\\034\\n\\016test_end_point\\022\\006string\\032\\002\\022\\000\\210\\001\\001\\n\\331\\001\\n GenerateBigQueryReaderPartitions\\032\\016\\n\\npartitions\\030\\007\\\"\\024\\n\\nproject_id\\022\\006string\\\"\\024\\n\\ndataset_id\\022\\006string\\\"\\022\\n\\010table_id\\022\\006string\\\"\\027\\n\\007columns\\022\\014list(string)\\\"\\027\\n\\020timestamp_millis\\022\\003int\\\"\\025\\n\\016num_partitions\\022\\003int\\\"\\034\\n\\016test_end_point\\022\\006string\\032\\002\\022\\000\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/tensorflow/contrib/cloud/python/ops/gen_bigquery_reader_ops.py\u001b[0m in \u001b[0;36m_InitOpDefLibrary\u001b[0;34m(op_list_proto_bytes)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \u001b[0mop_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m   \u001b[0mop_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_list_proto_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m   \u001b[0m_op_def_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_op_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m   \u001b[0mop_def_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_library\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpDefLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m   \u001b[0mop_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_op_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.framework.op_def_registry' has no attribute 'register_op_list'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile\n",
    "\n",
    "from tensorflow.contrib.cloud.python.ops.bigquery_reader_ops import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22424 total images.\n",
      "\n",
      "There are 17939 training images.\n",
      "There are 10 total training categories.\n",
      "There are 4485 validation images.\n",
      "There are 0 test images.\n"
     ]
    }
   ],
   "source": [
    "# function for loading datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    files = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']), 10)\n",
    "    return files, targets\n",
    "\n",
    "# loading train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('state-farm-distracted-driver-detection/imgs/train')\n",
    "test_files, test_targets = load_dataset('state-farm-distracted-driver-detection/imgs/test')\n",
    "\n",
    "\n",
    "# load list of names\n",
    "names = [item[17:19] for item in sorted(glob(\"state-farm-distracted-driver-detection/imgs/train/*/\"))]\n",
    "\n",
    "# break training set into training and validation sets\n",
    "train_files, valid_files, train_targets, valid_targets = train_test_split(train_files, train_targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# statistics regarding dataset\n",
    "print('There are %s total images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training images.' % len(train_files))\n",
    "print('There are %d total training categories.' % len(names))\n",
    "print('There are %d validation images.' % len(valid_files))\n",
    "print('There are %d test images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     c0\n",
      "1     c0\n",
      "2     c0\n",
      "3     c0\n",
      "4     c0\n",
      "      ..\n",
      "95    c1\n",
      "96    c1\n",
      "97    c1\n",
      "98    c1\n",
      "99    c1\n",
      "Name: classname, Length: 100, dtype: object\n",
      "count     22424\n",
      "unique       10\n",
      "top          c0\n",
      "freq       2489\n",
      "Name: classname, dtype: object\n",
      "\n",
      "label   #\n",
      "c9    2129\n",
      "c3    2346\n",
      "c4    2326\n",
      "c7    2002\n",
      "c2    2317\n",
      "c1    2267\n",
      "c0    2489\n",
      "c8    1911\n",
      "c5    2312\n",
      "c6    2325\n",
      "Name: classname, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"state-farm-distracted-driver-detection/driver_imgs_list.csv\",header='infer')\n",
    "print(df['classname'].head(100))\n",
    "print(df.iloc[:,1].describe())\n",
    "print(\"\\nlabel   #\")\n",
    "print(df['classname'].value_counts(sort=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "nf = df['classname'].value_counts(sort=False)\n",
    "labels = df['classname'].value_counts(sort=False).index.tolist()\n",
    "y = np.array(nf)\n",
    "width = 1/1.5\n",
    "N = len(y)\n",
    "x = range(N)\n",
    "\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "ay = fig.add_subplot(211)\n",
    "\n",
    "plt.xticks(x, labels, size=15)\n",
    "plt.yticks(size=15)\n",
    "\n",
    "ay.bar(x, y, width, color=\"blue\")\n",
    "\n",
    "plt.title('Bar Chart',size=25)\n",
    "plt.xlabel('classname',size=15)\n",
    "plt.ylabel('Count',size=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17939/17939 [01:41<00:00, 175.92it/s]\n",
      "100%|██████████| 4485/4485 [00:30<00:00, 148.34it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4299227d4e3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mvalid_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtest_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-8cafde67a779>\u001b[0m in \u001b[0;36mpaths_to_tensor\u001b[0;34m(img_paths)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlist_of_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# rescaling images by dividing every pixel in every image by 255. \n",
    "# subtract 0.5 to ensure the mean is zero\n",
    "                     \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255 - 0.5\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255 - 0.5\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255 - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# visulization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import gc # garbage collection\n",
    "import glob # extract path via pattern matching\n",
    "import random\n",
    "import math\n",
    "import cv2 # read image\n",
    "# store to disk\n",
    "import pickle\n",
    "import h5py # like numpy array\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Dense, Conv2D, MaxPool2D, AveragePooling2D\n",
    "from keras.layers import Flatten, Dropout, BatchNormalization, Activation\n",
    "from keras.layers import Add\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import regularizers\n",
    "import keras\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '../cs747-project/' #change the path\n",
    "TRAIN_DIR = ROOT_DIR + 'state-farm-distracted-driver-detection/imgs/train/'\n",
    "TEST_DIR = ROOT_DIR + 'state-farm-distracted-driver-detection/imgs/test/'\n",
    "driver_imgs_list = pd.read_csv(ROOT_DIR + \"state-farm-distracted-driver-detection/\" + \"driver_imgs_list.csv\")\n",
    "sample_submission = pd.read_csv(ROOT_DIR + \"state-farm-distracted-driver-detection/\" + \"sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(path, img_height=None, img_width=None, rotate=False, color_type=0):\n",
    "    img = cv2.imread(path, color_type)\n",
    "    if img_width and img_height:\n",
    "        img = cv2.resize(img, (img_width, img_height))\n",
    "    if rotate is True:\n",
    "        rows, cols = img.shape\n",
    "        rotation_angle = random.uniform(10,-10)\n",
    "        M = cv2.getRotationMatrix2D((cols/2, rows/2), rotation_angle, 1)\n",
    "        img = cv2.warpAffine(img, M, (cols,rows))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(480, 640)}\n"
     ]
    }
   ],
   "source": [
    "random_list = np.random.permutation(len(driver_imgs_list))[:50]\n",
    "df_copy = driver_imgs_list.iloc[random_list]\n",
    "image_paths = [TRAIN_DIR+row.classname+'/'+row.img \n",
    "                   for (index, row) in df_copy.iterrows()]\n",
    "image_shapes = [get_image(path).shape for path in image_paths]\n",
    "print(set(image_shapes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path_list = []\n",
    "label_list = []\n",
    "for index, row in driver_imgs_list.iterrows():\n",
    "    img_path_list.append('{0}{1}/{2}'.format(TRAIN_DIR, row.classname, row.img))\n",
    "    label_list.append(int(row.classname[1]))\n",
    "# One hot vector representation of labels\n",
    "y_labels_one_hot = to_categorical(label_list, dtype=np.int8)\n",
    "x_img_path = np.array(img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "np.save('x_img_path.npy', x_img_path)\n",
    "np.save('y_labels_one_hot.npy', y_labels_one_hot)\n",
    "\n",
    "x_img_path_shuffled, y_labels_one_hot_shuffled = shuffle(x_img_path, y_labels_one_hot)\n",
    "\n",
    "# saving the shuffled file.\n",
    "# you can load them later using np.load().\n",
    "np.save('y_labels_one_hot_shuffled.npy', y_labels_one_hot_shuffled)\n",
    "np.save('x_img_path_shuffled.npy', x_img_path_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17939,)\n",
      "(17939, 10)\n",
      "(4485,)\n",
      "(4485, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Used this line as our filename array is not a numpy array.\n",
    "x_img_path_shuffled_numpy = np.array(x_img_path_shuffled)\n",
    "\n",
    "X_train_filenames, X_val_filenames, y_train, y_val = train_test_split(\n",
    "    x_img_path_shuffled_numpy, y_labels_one_hot_shuffled, test_size=0.2, random_state=1)\n",
    "\n",
    "print(X_train_filenames.shape) # (3800,)\n",
    "print(y_train.shape)           # (3800, 12)\n",
    "\n",
    "print(X_val_filenames.shape)   # (950,)\n",
    "print(y_val.shape)             # (950, 12)\n",
    "\n",
    "# You can save these files as well. As you will be using them later for training and validation of your model.\n",
    "np.save('X_train_filenames.npy', X_train_filenames)\n",
    "np.save('y_train.npy', y_train)\n",
    "\n",
    "np.save('X_val_filenames.npy', X_val_filenames)\n",
    "np.save('y_val.npy', y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "BATCH_SIZE = 32\n",
    "CHANNEL = 3\n",
    "class Img_Generator(keras.utils.Sequence):\n",
    "    def __init__(self, image_filenames, labels, batch_size) :\n",
    "        self.image_filenames = image_filenames\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(np.int)\n",
    "    def __getitem__(self, idx) :\n",
    "        batch_x = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        img_list = []\n",
    "        for file_name in batch_x:\n",
    "            if CHANNEL == 1:\n",
    "                original_img = cv2.imread(file_name, 0)\n",
    "            else:\n",
    "                original_img = cv2.imread(file_name, 1)\n",
    "            im = cv2.resize(original_img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "            #color = [0, 0, 0]\n",
    "            #new_im = cv2.copyMakeBorder(im, 40, 40, 0, 0, cv2.BORDER_CONSTANT, value=color)\n",
    "            #im = cv2.resize(new_im, (224, 224))\n",
    "            img_list.append(im)\n",
    "        img_batch = np.array(img_list)\n",
    "        if CHANNEL == 1:\n",
    "            img_batch = np.expand_dims(img_batch, axis=-1)\n",
    "        return img_batch, np.array(batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of input [BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH, CHANNEL]\n",
    "train_gen = Img_Generator(X_train_filenames, y_train, BATCH_SIZE)\n",
    "val_gen = Img_Generator(X_val_filenames, y_val, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f saved_models/weights_best_efficient.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-8465e4f1d8af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     'releases/download/efficientnet/')\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0meff_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEfficientNetB5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0min_lay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/efficientnet/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/efficientnet/model.py\u001b[0m in \u001b[0;36mEfficientNetB5\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpooling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m     )\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/efficientnet/model.py\u001b[0m in \u001b[0;36mEfficientNet\u001b[0;34m(width_coefficient, depth_coefficient, default_resolution, dropout_rate, drop_connect_rate, depth_divisor, blocks_args, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m                       \u001b[0muse_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                       \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONV_KERNEL_INITIALIZER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                       name='stem_conv')(x)\n\u001b[0m\u001b[1;32m    351\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbn_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stem_bn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'stem_activation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                 \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m                 raise ValueError('Layer ' + self.name + ' was called with '\n",
      "\u001b[0;32m/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \"\"\"\n\u001b[0;32m--> 695\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[1;32m    697\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/snowflakes/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mis_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TensorLike\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dense_tensor_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.python.framework.ops' has no attribute '_TensorLike'"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "from efficientnet.keras import EfficientNetB5\n",
    "from keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n",
    "                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D,concatenate,Concatenate,multiply, LocallyConnected2D, Lambda)\n",
    "\n",
    "nb_train_samples = 17943\n",
    "nb_validation_samples = 4481\n",
    "\n",
    "IMAGENET_WEIGHTS_HASHES = {\n",
    "    'efficientnet-b5': ('30172f1d45f9b8a41352d4219bf930ee'\n",
    "                        '3339025fd26ab314a817ba8918fefc7d',\n",
    "                        '9d197bc2bfe29165c10a2af8c2ebc675'\n",
    "                        '07f5d70456f09e584c71b822941b1952')\n",
    "}\n",
    "IMAGENET_WEIGHTS_PATH = (\n",
    "    'https://github.com/Callidior/keras-applications/'\n",
    "    'releases/download/efficientnet/')\n",
    "\n",
    "eff_net = EfficientNetB5(weights= None, include_top=False, input_shape=(64, 64, 3))\n",
    "in_lay = Input(shape=(64,64,3))\n",
    "\n",
    "file_name = \"efficientnet-b5\" + '_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5'\n",
    "file_hash = IMAGENET_WEIGHTS_HASHES[\"efficientnet-b5\"][1]\n",
    "weights_path = keras.utils.get_file(\n",
    "            file_name,\n",
    "            IMAGENET_WEIGHTS_PATH + file_name,\n",
    "            cache_subdir='models',\n",
    "            file_hash=file_hash,\n",
    " )\n",
    "eff_net.load_weights(weights_path)\n",
    "pt_depth = eff_net.get_output_shape_at(0)[-1]\n",
    "pt_features = eff_net(in_lay)\n",
    "bn_features = BatchNormalization()(pt_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import (Activation, Dropout, Flatten, Dense, GlobalMaxPooling2D,\n",
    "                          BatchNormalization, Input, Conv2D, GlobalAveragePooling2D,concatenate,Concatenate,multiply, LocallyConnected2D, Lambda)\n",
    "# here we do an attention mechanism to turn pixels in the GAP on an off\n",
    "attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(Dropout(0.5)(bn_features))\n",
    "attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
    "attn_layer = Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
    "attn_layer = Conv2D(1, kernel_size = (1,1), padding = 'valid', activation = 'sigmoid')(attn_layer)\n",
    "\n",
    "mask_features = multiply([attn_layer, bn_features])\n",
    "gap_features = GlobalAveragePooling2D()(mask_features)\n",
    "gap_mask = GlobalAveragePooling2D()(attn_layer)\n",
    "# to account for missing values from the attention model\n",
    "gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n",
    "gap_dr = Dropout(0.25)(gap)\n",
    "dr_steps = Dropout(0.25)(Dense(128, activation = 'relu')(gap_dr))\n",
    "out_layer = Dense(10, activation = 'softmax')(dr_steps)\n",
    "retina_model = Model(inputs = [in_lay], outputs = [out_layer])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
